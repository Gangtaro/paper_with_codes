{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# from .modules import Encoder, LayerNorm\n",
    "from modules import Encoder, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_items = [[0, 0, 1,2,3,4,5,6], [0, 0, 0, 0,3,4,5,6]]\n",
    "input_ratings = [[0, 0, .1, .2, .4, .2, .1, .8], [0, 0, 0, 0, .4, .2, .1, .8]]\n",
    "\n",
    "input_items = torch.tensor(input_items)\n",
    "input_ratings = torch.tensor(input_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1]])\n",
      "tensor([[[[0, 0, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0, 0, 1, 1, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = (input_items > 0).long()\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # torch.int64\n",
    "\n",
    "print(attention_mask)\n",
    "print(extended_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000, 0.8000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.2000, 0.1000, 0.8000]])\n",
      "tensor([[[[0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000, 0.8000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.2000, 0.1000, 0.8000]]]])\n"
     ]
    }
   ],
   "source": [
    "weighted_mask = input_ratings\n",
    "extended_weighted_mask = weighted_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(weighted_mask)\n",
    "print(extended_weighted_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "max_len = attention_mask.size(-1)\n",
    "attn_shape = (1, max_len, max_len)\n",
    "\n",
    "print(max_len)\n",
    "print(attn_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "------------------------------------------------------------------------------\n",
      "tensor([[[[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 1, 1, 0, 0, 0, 0],\n",
      "          [0, 0, 1, 1, 1, 0, 0, 0],\n",
      "          [0, 0, 1, 1, 1, 1, 0, 0],\n",
      "          [0, 0, 1, 1, 1, 1, 1, 0],\n",
      "          [0, 0, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 1, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 1, 1, 0, 0],\n",
      "          [0, 0, 0, 0, 1, 1, 1, 0],\n",
      "          [0, 0, 0, 0, 1, 1, 1, 1]]]])\n",
      "------------------------------------------------------------------------------\n",
      "tensor([[[[-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000.,     -0., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000.,     -0.,     -0., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000.,     -0.,     -0.,     -0., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000.,     -0.,     -0.,     -0.,     -0., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "           -10000.],\n",
      "          [-10000., -10000.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
      "               -0.]]],\n",
      "\n",
      "\n",
      "        [[[-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000.,     -0., -10000., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000.,     -0.,     -0., -10000.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000.,     -0.,     -0.,     -0.,\n",
      "           -10000.],\n",
      "          [-10000., -10000., -10000., -10000.,     -0.,     -0.,     -0.,\n",
      "               -0.]]]])\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)\n",
    "subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
    "subsequent_mask = subsequent_mask.long()\n",
    "\n",
    "print(subsequent_mask)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "\n",
    "extended_attention_mask = extended_attention_mask * subsequent_mask \n",
    "\n",
    "print(extended_attention_mask)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "print(extended_attention_mask)\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000, 0.8000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.2000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.2000, 0.1000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.2000, 0.1000, 0.8000]]]])\n"
     ]
    }
   ],
   "source": [
    "weighted_mask_by_score = extended_weighted_mask * subsequent_mask\n",
    "\n",
    "print(weighted_mask_by_score)\n",
    "\n",
    "# 이 부분 과연 해당 Rating으로 weight를 곱해주는 것이 올바른 선택일까? \n",
    "\n",
    "# Test 해볼 수 있는 여지는 2가지가 있다\n",
    "####################################################################################\n",
    "#  - 1 ) 학습은 implict과 비슷한 구조를 채택하고 오로지 loss 값을 구할 때만, Explicit 정보를 활용한다. \n",
    "    # 이렇게 학습하면, 학습시에 Explicit한 정답은 오로지 외부에서만 참조가 되므로, \n",
    "    # 추론시에는 오로지 긍, 부정으로 선택한 결과에 대해 긍정만 시퀀스로 입력해줄 수 있는 방법이 있다. \n",
    "\n",
    "####################################################################################\n",
    "# - 2 ) masking이 0\n",
    "# masking을 0으로 처리한 것에 대해서 생각해보자\n",
    "# 그렇게 나온 값에 대해서 학습에 사용하지 않는다는 것 = 해당 위치에 있는 아이템에 대해서 참조하지 않겠다는 것이다. \n",
    "# 유저의 점수가 0에서 1사이 일때, 이 값을 관심도라고 하고 해당 위치 아이템의 선호도를 통해서 어텐션 스코어에 유저의 관심도의 역할로 한 번 더 처리해주는 역할을 하게 된다. \n",
    "\n",
    "# [] 여기서는 일단 2번 수행 방법을 통해서 접근하게 된다. \n",
    "# 확실한 것은, 스코어를 유저의 관심도로 표현함에 따른 리스크\n",
    "# 점수에 대한 유저의 평가를 각각 표준화 시켜야한다는 것이다. ####### -> 이런 가정 조건이 없다면, 유저가 평가한 점수가, 각각 지멋대로이기 때문에, 균일한 학습이 잘 되지 않을 수 있다. \n",
    "# 따라서 해당 부분은 유저별로 다르게 놔두는것이 아니라 같은 분포를 따르도록 만드는 것이 중요할 것으로 판단된다. \n",
    "# 왜냐면 해당 모델은 유저가 구분되지 않는 시퀀셜한 정보만으로 해당 시퀀스의 성격을 판단하게 되므로, \n",
    "# 유저별 기록의 편차를 줄여주어야 모델이 조금 더 쉽게 학습시킬 수 있을 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0a9b01602dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweighted_mask_by_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweighted_mask_by_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "weighted_mask_by_score.size()\n",
    "\n",
    "torch.tensor([[0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 1, 1, 1, 1, 1, 1]]) * weighted_mask_by_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "          [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "          [0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "subsequent_mask = torch.tril(torch.ones(attn_shape), diagonal=1)\n",
    "subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
    "subsequent_mask = subsequent_mask.long()\n",
    "print(subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f987112d13c3a5109fc24d7b4f4d6f95510a722443984c5c3a93525168aad10"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.final': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
